{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gemma 2 2b-it 모델 파인튜닝 해보기\n",
    "https://devocean.sk.com/blog/techBoardDetail.do?ID=165703&boardType=techBlog\n",
    "\n",
    "    - data_set : nlpai-lab/KULLM - huggingface\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 성능 모니터링\n",
    "```bash\n",
    "$ sudo apt install htop  # Ubuntu/Debian 기반\n",
    "$ htop\n",
    "```\n",
    "```bash\n",
    "$ sudo apt install glances\n",
    "$ glances\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U transformers datasets bitsandbytes peft trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 정리 \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments, GemmaTokenizerFast\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# gemma 2 2b-it 모델을 기반하여 학습 시킨다.\n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "\n",
    "### 모델 실행 시 주의 한 번만 실행 요망 ###\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4521, 736, 603, 476, 2121]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GemmaTokenizerFast\n",
    "\n",
    "# 토크나이저 결과\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "tokenizer.encode(\"Hello this is a test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma2 - 2b - it 대화 형식 \n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "Write a hello world program<end_of_turn>\n",
    "<start_of_turn>model\n",
    "```\n",
    "    － <bos> ： Beginning of sequence\n",
    "    － <start_of_turn> ： 시작 구분 기호\n",
    "    － user : 엔티티의 역할\n",
    "    － <end_of_turn> : 종료 구분 기호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 정리 \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments, GemmaTokenizerFast\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'instruction', 'input', 'output'],\n",
       "    num_rows: 152630\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드 : KULLM이라는 LLM의 학습 데이터셋 불러오기\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('nlpai-lab/kullm-v2', \n",
    "                       split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'vicuna_{idx}',\n",
       " 'instruction': '항바이러스제는 생명을 위협하는 바이러스에만 사용되나요?',\n",
       " 'input': '',\n",
       " 'output': '항바이러스제는 생명을 위협하는 바이러스에만 사용되는 것이 아닙니다. 경증 및 중증 바이러스를 포함한 광범위한 바이러스 감염을 치료하는 데 사용할 수 있습니다. 항바이러스제 사용 여부는 일반적으로 특정 바이러스, 감염의 중증도, 환자의 전반적인 건강 상태에 따라 결정됩니다.예를 들어, 항바이러스제는 인플루엔자(독감) 치료에 자주 사용되는데, 인플루엔자는 노인, 어린이, 특정 기저질환이 있는 사람 등 특정 사람들에게는 심각하고 심지어 생명을 위협하는 질병이 될 수 있습니다. 항바이러스제는 독감의 중증도와 기간을 줄이는 데 도움이 될 수 있으며, 폐렴과 같은 심각한 합병증을 예방하는 데도 도움이 될 수 있습니다.다른 경우에는 항바이러스제를 사용하여 생명을 위협하지는 않지만 심각한 불편함을 유발하고 일상 활동에 지장을 줄 수 있는 바이러스 감염을 치료할 수 있습니다. 예를 들어, 항바이러스제는 피부나 점막에 통증성 궤양을 유발할 수 있는 단순포진 바이러스(HSV) 감염을 치료하는 데 사용될 수 있습니다.전반적으로 바이러스 감염을 치료하기 위해 항바이러스제를 사용할지 여부는 특정 바이러스, 감염의 심각성, 치료의 잠재적 위험과 이점을 포함한 다양한 요인에 따라 결정됩니다.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('3원색이란 무엇인가요?',\n",
       " '세 가지 기본 색은 빨강, 파랑, 노랑입니다. 이 색은 다른 색을 혼합하여 만들 수 없고 다른 모든 색은 다양한 비율로 조합하여 만들 수 있기 때문에 원색이라고 부릅니다. 빛에 사용되는 첨가제 색상 시스템에서 원색은 빨강, 녹색, 파랑(RGB)입니다.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질문 / 대답\n",
    "dataset['instruction'][0], dataset['output'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습용 프롬프트 조정\n",
    "\n",
    "<bos><start_of_turn>user\n",
    "{본문} <end_of_turn>\n",
    "<start_of_turn>model\n",
    "{대답} <end_of_turn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(example):\n",
    "    prompt_list = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
    "사용자의 질문 입니다. 적절한 답변을 해주세요:\n",
    "\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{}<end_of_turn><eos>\"\"\".format(example['instruction'][i], example['output'][i]))\n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n3원색이란 무엇인가요?<end_of_turn>\\n<start_of_turn>model\\n세 가지 기본 색은 빨강, 파랑, 노랑입니다. 이 색은 다른 색을 혼합하여 만들 수 없고 다른 모든 색은 다양한 비율로 조합하여 만들 수 있기 때문에 원색이라고 부릅니다. 빛에 사용되는 첨가제 색상 시스템에서 원색은 빨강, 녹색, 파랑(RGB)입니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n어려운 결정을 내려야 했던 순간을 설명해 주세요.<end_of_turn>\\n<start_of_turn>model\\nAI 어시스턴트인 저는 저만의 개인적인 경험이나 물리적 존재를 가지고 있지 않습니다. 하지만 알고리즘, 논리, 데이터를 기반으로 의사 결정을 내리도록 프로그래밍되어 있습니다. 상충되는 정보나 여러 가지 가능한 결과가 있을 수 있기 때문에 의사결정이 어려운 상황에 직면할 때가 있습니다. 그런 경우 저는 각 옵션의 위험과 이점을 분석하고 비교하여 원하는 결과를 얻을 가능성이 가장 높은 결론에 도달할 수 있는 능력을 활용합니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n원자의 구조를 설명합니다.<end_of_turn>\\n<start_of_turn>model\\n원자는 모든 물질의 기본 구성 요소이며 양성자, 중성자, 전자의 세 가지 유형의 입자로 이루어져 있습니다. 원자의 구조는 전자 구름으로 둘러싸인 중앙에 핵이 있는 것으로 설명할 수 있습니다.\\n\\n원자의 핵은 양성자와 중성자로 구성됩니다. 양성자는 양전하를 띠는 입자이고 중성자는 전하를 띠지 않는 중성 입자입니다. 이 두 입자는 원자의 중심에 있으며 원자 질량의 대부분을 차지하는 원자핵에 위치합니다.\\n\\n원자핵을 둘러싸고 있는 것은 전자 구름입니다. 전자는 음전하를 띠는 입자로, 원자핵 주변에서 끊임없이 움직입니다. 전자 구름은 껍질 또는 궤도로 나뉘며, 각 껍질은 특정 수의 전자를 보유할 수 있습니다. 원자가 껍질이라고 하는 가장 바깥쪽 껍질에 있는 전자의 수에 따라 원자의 화학적 특성이 결정됩니다.\\n\\n중성 원자에서 핵의 양성자 수는 전자 구름의 전자 수와 같으므로 양전하와 음전하가 균형을 이루며 원자는 전체 전하를 갖지 않습니다. 원자 번호라고도 하는 양성자의 수는 원자가 어떤 원소인지 결정합니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n대기 오염을 줄이려면 어떻게 해야 할까요?<end_of_turn>\\n<start_of_turn>model\\n대기 오염을 줄이는 방법에는 다음과 같은 여러 가지가 있습니다:\\n\\n1. 에너지 소비를 줄입니다: 에너지를 절약하면 발전소에서 배출되는 오염 물질의 양을 줄일 수 있습니다. 에너지 효율이 높은 가전제품과 조명을 사용하고, 집을 단열하며, 사용하지 않을 때는 조명과 전자기기를 끄는 등의 방법이 있습니다.\\n\\n2. 대중교통 또는 카풀 이용하기: 자동차와 트럭은 주요 오염원입니다. 대중교통, 카풀 또는 자전거를 이용하면 도로에 운행되는 자동차 수를 줄여 배기가스 배출량을 줄일 수 있습니다.\\n\\n3. 폐기물을 책임감 있게 처리하세요: 폐기물이 태워지거나 분해되면 유해 가스가 대기 중으로 방출될 수 있습니다. 유해 폐기물을 올바르게 처리하고 플라스틱, 종이, 유리와 같은 재료는 가능하면 재활용하세요.\\n\\n4. 청정 에너지원을 지원하세요: 태양열 및 풍력과 같은 재생 에너지원은 오염을 거의 또는 전혀 일으키지 않습니다. 청정 에너지 회사 및 프로젝트를 지원하고 집에 재생 에너지 시스템을 설치하는 것을 고려하세요.\\n\\n5. 효율적으로 운전하기: 운전을 해야 하는 경우, 차량을 잘 관리하고 급가속과 급제동을 피하고 공회전을 줄이는 등 효율적으로 운전하세요.\\n\\n6. 제품 선택에 유의하세요: 일부 제품과 관행은 다른 제품보다 오염 물질을 더 많이 배출합니다. 친환경 포장재를 사용한 제품을 선택하고 유해 화학 물질을 대기 중으로 방출할 수 있는 에어로졸 스프레이 사용을 최소화하세요.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n건강을 유지하기 위한 세 가지 팁을 알려주세요.<end_of_turn>\\n<start_of_turn>model\\n1. 균형 잡힌 영양가 있는 식단을 섭취합니다: 식단에 다양한 과일과 채소, 저지방 단백질, 통곡물, 건강한 지방이 포함되어 있는지 확인하세요. 이렇게 하면 신체가 최상의 기능을 발휘하는 데 필요한 필수 영양소를 공급하고 만성 질환을 예방하는 데 도움이 됩니다.\\n\\n2. 규칙적인 신체 활동을 합니다: 운동은 뼈, 근육, 심혈관 건강을 튼튼하게 유지하는 데 매우 중요합니다. 매주 150분 이상의 중간 강도의 유산소 운동 또는 75분 이상의 격렬한 운동을 목표로 하세요.\\n\\n3. 충분한 수면을 취합니다: 양질의 수면을 충분히 취하는 것은 신체적, 정신적 건강에 매우 중요합니다. 수면은 기분을 조절하고 인지 기능을 개선하며 건강한 성장과 면역 기능을 지원하는 데 도움이 됩니다. 매일 밤 7~9시간 수면을 목표로 하세요.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n물의 끓는점을 찾아보세요.<end_of_turn>\\n<start_of_turn>model\\n물의 끓는점은 표준 대기압에서 섭씨 100도 또는 화씨 212도입니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n주어진 숫자를 오름차순으로 정렬합니다.<end_of_turn>\\n<start_of_turn>model\\n0, 2, 3, 4, 8.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n세계에서 가장 유명한 화가는 누구인가요?<end_of_turn>\\n<start_of_turn>model\\n세계에서 가장 유명한 화가의 칭호는 주관적이며 개인의 의견에 따라 달라질 수 있습니다. 하지만 많은 사람들이 이탈리아 르네상스 시대의 유명한 예술가인 레오나르도 다빈치를 세계에서 가장 유명한 화가 중 한 명으로 꼽습니다. 그는 \"모나리자\"와 \"최후의 만찬\"을 비롯한 그의 상징적인 작품으로 유명합니다. 그 외에도 빈센트 반 고흐, 파블로 피카소, 미켈란젤로 등 세계에서 가장 유명한 화가 중 한 명으로 꼽히는 유명 화가들이 있습니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n컴퓨터 마더보드의 기능 설명하기<end_of_turn>\\n<start_of_turn>model\\n메인보드 또는 시스템 보드라고도 하는 마더보드는 컴퓨터의 중앙 인쇄 회로 기판입니다. 마더보드는 컴퓨터의 백본 또는 기초 역할을 하며 CPU, RAM, 스토리지 드라이브, 확장 카드 및 주변 장치와 같은 다양한 구성 요소를 모두 연결합니다. 마더보드는 이러한 구성 요소 간의 통신 및 데이터 전송을 관리하여 함께 작동하고 지정된 작업을 수행할 수 있도록 합니다.\\n\\n또한 마더보드에는 여러 구성 요소에 전원을 공급하는 전원 조절 회로와 이러한 구성 요소의 작동을 동기화하는 클럭 생성기와 같은 중요한 회로가 포함되어 있습니다. 또한 부팅 프로세스를 제어하고 컴퓨터 하드웨어를 구성하고 관리하기 위한 인터페이스를 제공하는 펌웨어인 BIOS(기본 입력/출력 시스템)도 포함되어 있습니다. 마더보드의 다른 기능에는 내장 네트워킹, 오디오 및 비디오 기능이 포함될 수 있습니다.\\n\\n전반적으로 컴퓨터 마더보드의 기능은 컴퓨터를 구성하는 모든 다양한 구성 요소의 통합 및 작동을 위한 플랫폼을 제공하는 것으로, 모든 컴퓨팅 시스템의 필수적인 부분입니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n다음 문장을 수정하여 더 간결하게 만듭니다.<end_of_turn>\\n<start_of_turn>model\\n그는 5분 후에 도착하는 버스를 타기 위해 버스 정류장으로 달려갔습니다.<end_of_turn><eos>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prompt(dataset[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemma 양자화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정: 대규모 언어 모델의 특정 레이어에서만 파라미터를 미세 조정하여\n",
    "# 메모리 사용량을 줄이고 학습 효율성을 높임\n",
    "lora_config = LoraConfig(\n",
    "    r=6,  # LoRA에서 사용되는 저차원 공간의 랭크(r) 값. 값이 작을수록 학습할 파라미터가 적어짐\n",
    "    lora_alpha=8,  # LoRA의 학습 속도 조절을 위한 스케일링 파라미터. 값이 클수록 학습 변동폭이 커짐\n",
    "    lora_dropout=0.05,  # 드롭아웃 확률 설정 (5%). 과적합 방지를 위해 일부 노드를 무작위로 제외\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  \n",
    "    # LoRA가 적용될 트랜스포머 모델의 특정 레이어들 (쿼리, 출력, 키, 값 등의 투영 레이어)\n",
    "    task_type=\"CAUSAL_LM\",  # Causal Language Modeling 작업 유형 (GPT 모델이 주로 사용)\n",
    ")\n",
    "\n",
    "# BitsAndBytes 설정: 모델을 4비트로 양자화하여 메모리 사용량을 줄이고 성능 최적화\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 모델을 4비트로 로드하여 메모리 효율성을 극대화함\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NF4(Normalized Float 4) 방식의 4비트 양자화 사용\n",
    "    bnb_4bit_compute_dtype=torch.float16  # 계산에 사용할 데이터 타입을 float16으로 설정 (16비트 부동소수점)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕? 내이름이 뭔지 대답 해줘.\n",
      "\n",
      "**Answer:**  My name is Bard. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# gemma 2 2b-it 모델을 기반하여 학습 시킨다.\n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # 자동으로 GPU에 할당\n",
    ")\n",
    "\n",
    "# 입력 텍스트 처리\n",
    "input_text = \"안녕? 내이름이 뭔지 대답 해줘\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # 입력을 GPU로 이동\n",
    "\n",
    "# 모델을 통해 출력 생성\n",
    "outputs = model.generate(input_ids['input_ids'], max_new_tokens=32)\n",
    "\n",
    "# 출력 디코딩 및 표시\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, \n",
    "                                             device_map=\"auto\",  # 이 줄이 중요\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             attn_implementation='eager'\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 152630/152630 [00:36<00:00, 4180.92 examples/s]\n",
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "#        num_train_epochs = 1,\n",
    "        max_steps=3000,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_steps = 1000,\n",
    "        learning_rate=3e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=generate_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 2:12:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.428900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.377000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.362700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.332200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=1.4303289031982422, metrics={'train_runtime': 7944.0505, 'train_samples_per_second': 1.511, 'train_steps_per_second': 0.378, 'total_flos': 5.066762968044288e+16, 'train_loss': 1.4303289031982422, 'epoch': 0.07862150298106532})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/transformers/modeling_utils.py:2633: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n",
      "Saving checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.52s/it]\n"
     ]
    }
   ],
   "source": [
    "ADAPTER_MODEL = \"lora_adapter\"\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_MODEL)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained('gemma-2b-it-sum-ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 30M\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4.0K Sep 23 13:06 .\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4.0K Sep 23 13:06 ..\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 5.0K Sep 23 13:06 README.md\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  721 Sep 23 13:06 adapter_config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  30M Sep 23 13:06 adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "! ls -alh lora_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.9G\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4.0K Sep 23 13:06 .\n",
      "drwxrwxr-x 6 ubuntu ubuntu 4.0K Sep 23 13:06 ..\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  880 Sep 23 13:06 config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  187 Sep 23 13:06 generation_config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 4.7G Sep 23 13:06 model-00001-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 230M Sep 23 13:06 model-00002-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  24K Sep 23 13:06 model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "! ls -alh ./gemma-2b-it-sum-ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned 모델 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "FINETUNE_MODEL = \"./gemma-2b-it-sum-ko\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 모델을 4비트로 로드하여 메모리 효율성을 극대화함\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NF4(Normalized Float 4) 방식의 4비트 양자화 사용\n",
    "    bnb_4bit_compute_dtype=torch.float16  # 계산에 사용할 데이터 타입을 float16으로 설정 (16비트 부동소수점)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", quantization_config=bnb_config)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘의 날씨가 맑으면 여러 가지 이점이 있습니다. 햇빛이 많이 들어오는 것은 우리의 기분을 개선하고 에너지를 끌어올릴 수 있습니다. 또한 맑은 날씨는 산책이나 운동을 하기 좋은 날이 되어 활동에 활력을 불어넣을 수 있습니다. 또한 맑은 날씨는 우리의 기분을 개선하고 스트레스를 줄이는 데 도움이 될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = '오늘의 날씨는 맑았어 좋은 점이 뭘까?'\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n{}\".format(text)\n",
    "    }\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = pipe_finetuned(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env310_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
