{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gemma 2 2b-it 모델 파인튜닝 해보기\n",
    "https://devocean.sk.com/blog/techBoardDetail.do?ID=165703&boardType=techBlog\n",
    "\n",
    "    - data_set : nlpai-lab/KULLM - huggingface\n",
    "    - Model : google/gemma-2-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운 받을 모듈 정리\n",
    "! pip install -q -U transformers datasets bitsandbytes peft trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 모듈 정리 \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments, GemmaTokenizerFast\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로딩 및 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 셋 종류 : dict_keys(['id', 'instruction', 'input', 'output'])\n",
      "id 종류 : {'alpaca_{idx}', 'vicuna_{idx}', 'dolly_{idx}'}\n",
      "alpaca_{idx} 퍼센트 : 34.07 %\n",
      "vicuna_{idx} 퍼센트 : 56.1 %\n",
      "dolly_{idx} 퍼센트 : 9.83 %\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드 : KULLM이라는 LLM의 학습 데이터셋 불러오기\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('nlpai-lab/kullm-v2', split=\"train\")\n",
    "print(\"데이터 셋 종류 :\", dataset.features.keys())\n",
    "print(\"id 종류 :\", set(dataset['id']))\n",
    "for key in list(set(dataset['id'])):\n",
    "    print(f\"{str(key)} 퍼센트 :\", round(len([i for i in dataset if i['id'] ==str(key)])/len(dataset) * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset['instruction'][0] >>> \n",
      "\t 3원색이란 무엇인가요?\n",
      "dataset['output'][0] >>> \n",
      "\t 세 가지 기본 색은 빨강, 파랑, 노랑입니다. 이 색은 다른 색을 혼합하여 만들 수 없고 다른 모든 색은 다양한 비율로 조합하여 만들 수 있기 때문에 원색이라고 부릅니다. 빛에 사용되는 첨가제 색상 시스템에서 원색은 빨강, 녹색, 파랑(RGB)입니다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예시 \n",
    "print(\"dataset['instruction'][0] >>> \\n\\t\", dataset['instruction'][0])\n",
    "print(\"dataset['output'][0] >>> \\n\\t\", dataset['output'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 형식 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n3원색이란 무엇인가요?<end_of_turn>\\n<start_of_turn>model\\n세 가지 기본 색은 빨강, 파랑, 노랑입니다. 이 색은 다른 색을 혼합하여 만들 수 없고 다른 모든 색은 다양한 비율로 조합하여 만들 수 있기 때문에 원색이라고 부릅니다. 빛에 사용되는 첨가제 색상 시스템에서 원색은 빨강, 녹색, 파랑(RGB)입니다.<end_of_turn><eos>',\n",
       " '<bos><start_of_turn>user\\n사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n어려운 결정을 내려야 했던 순간을 설명해 주세요.<end_of_turn>\\n<start_of_turn>model\\nAI 어시스턴트인 저는 저만의 개인적인 경험이나 물리적 존재를 가지고 있지 않습니다. 하지만 알고리즘, 논리, 데이터를 기반으로 의사 결정을 내리도록 프로그래밍되어 있습니다. 상충되는 정보나 여러 가지 가능한 결과가 있을 수 있기 때문에 의사결정이 어려운 상황에 직면할 때가 있습니다. 그런 경우 저는 각 옵션의 위험과 이점을 분석하고 비교하여 원하는 결과를 얻을 가능성이 가장 높은 결론에 도달할 수 있는 능력을 활용합니다.<end_of_turn><eos>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prompt(example):\n",
    "    prompt_list = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
    "사용자의 질문 입니다. 적절한 답변을 해주세요:\n",
    "\n",
    "{}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{}<end_of_turn><eos>\"\"\".format(example['instruction'][i], example['output'][i]))\n",
    "    return prompt_list\n",
    "\n",
    "generate_prompt(dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gemma 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# BitsAndBytes 설정: 모델을 4비트로 양자화하여 메모리 사용량을 줄이고 성능 최적화\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16  \n",
    ")\n",
    "\n",
    "# 베이스 모델 \n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# 토크나이저 설정 \n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# 모델 설정 \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL, quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # 자동으로 GPU에 할당\n",
    "    attn_implementation='eager' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# LoRA 설정: 대규모 언어 모델의 특정 레이어에서만 파라미터를 미세 조정하여\n",
    "# 메모리 사용량을 줄이고 학습 효율성을 높임\n",
    "lora_config = LoraConfig(\n",
    "    r=6,  \n",
    "    lora_alpha=8,  \n",
    "    lora_dropout=0.05,  \n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  \n",
    "    task_type=\"CAUSAL_LM\",  \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"outputs\",\n",
    "#        num_train_epochs = 1,\n",
    "        max_steps=3000,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        warmup_steps = 1000,\n",
    "        learning_rate=3e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=generate_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 2:11:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.406700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.400100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.363400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.350300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.332400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=1.4303758595784506, metrics={'train_runtime': 7870.7505, 'train_samples_per_second': 1.525, 'train_steps_per_second': 0.381, 'total_flos': 5.066762968044288e+16, 'train_loss': 1.4303758595784506, 'epoch': 0.07862150298106532})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 학습하기 ### \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-2-2b-FT(feat.kullm)/tokenizer_config.json',\n",
       " 'gemma-2-2b-FT(feat.kullm)/special_tokens_map.json',\n",
       " 'gemma-2-2b-FT(feat.kullm)/tokenizer.model',\n",
       " 'gemma-2-2b-FT(feat.kullm)/added_tokens.json',\n",
       " 'gemma-2-2b-FT(feat.kullm)/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 저장하기 ###\n",
    "LORA_SAVE_NAME = \"gemma-2-2b-FT-lr\"\n",
    "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
    "\n",
    "trainer.model.save_pretrained(LORA_SAVE_NAME)\n",
    "trainer.tokenizer.save_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 51M\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4.0K Sep 24 11:04 .\n",
      "drwxrwxr-x 7 ubuntu ubuntu 4.0K Sep 24 11:04 ..\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 5.0K Sep 24 11:04 README.md\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  721 Sep 24 11:04 adapter_config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  30M Sep 24 11:04 adapter_model.safetensors\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  636 Sep 24 11:04 special_tokens_map.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  17M Sep 24 11:04 tokenizer.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 4.1M Sep 24 11:04 tokenizer.model\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  46K Sep 24 11:04 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "! ls -alh gemma-2-2b-FT\\(feat.kullm\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.9G\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4.0K Sep 23 13:06 .\n",
      "drwxrwxr-x 7 ubuntu ubuntu 4.0K Sep 24 11:04 ..\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  880 Sep 23 13:06 config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  187 Sep 23 13:06 generation_config.json\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 4.7G Sep 23 13:06 model-00001-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 230M Sep 23 13:06 model-00002-of-00002.safetensors\n",
      "-rw-rw-r-- 1 ubuntu ubuntu  24K Sep 23 13:06 model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "! ls -alh ./gemma-2b-it-sum-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/kkw_env1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BASE_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(FINETUNE_MODEL)\n\u001b[1;32m     12\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     13\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 모델을 4비트로 로드하여 메모리 효율성을 극대화함\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# NF4(Normalized Float 4) 방식의 4비트 양자화 사용\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16  \u001b[38;5;66;03m# 계산에 사용할 데이터 타입을 float16으로 설정 (16비트 부동소수점)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mBASE_MODEL\u001b[49m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantization_config\u001b[38;5;241m=\u001b[39mbnb_config)\n\u001b[1;32m     19\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     20\u001b[0m pipe_finetuned \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mfinetune_model, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BASE_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "### 모델 불러오기 ### \n",
    "\n",
    "FINETUNE_MODEL = \"gemma-2-2b-FT(feat.kullm)\"\n",
    "\n",
    "finetune_model = AutoModelForCausalLM.from_pretrained(FINETUNE_MODEL, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINETUNE_MODEL)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 모델을 4비트로 로드하여 메모리 효율성을 극대화함\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NF4(Normalized Float 4) 방식의 4비트 양자화 사용\n",
    "    bnb_4bit_compute_dtype=torch.float16  # 계산에 사용할 데이터 타입을 float16으로 설정 (16비트 부동소수점)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", quantization_config=bnb_config)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "pipe_finetuned = pipeline(\"text-generation\", model=finetune_model, tokenizer=tokenizer, max_new_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### 추론 ###\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\",\n",
    "    mapping = {\"role\": \"from\", \"content\":\"value\", \"user\": \"human\", \"assistant\":\"gpt\"},\n",
    ")\n",
    "\n",
    "# 모델을 2배 빠른 추론 모드로 전환\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# 입력 메시지 설정\n",
    "text = '오늘의 날씨는 맑았어 좋은 점이 뭘까?'\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"사용자의 질문 입니다. 적절한 답변을 해주세요:\\n\\n{}\".format(text)\n",
    "    }\n",
    "]\n",
    "\n",
    "# 입력 데이터를 텐서로 변환후 gpu 전송\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors=\"pt\" # 파이토치 텐서로 변환\n",
    ").to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkw_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
